{"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.4"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ndef build_cnn(filter_list, kernel_dim, act_func, dense_units, class_count=10, input_depth=3):\n    network_layers = []\n    prev_channels = input_depth\n    \n    for num_filters in filter_list:\n        network_layers.append(nn.Conv2d(prev_channels, num_filters, kernel_size=kernel_dim, padding=1))\n        network_layers.append(select_activation(act_func))\n        network_layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n        prev_channels = num_filters\n    \n    conv_net = nn.Sequential(*network_layers)\n    flatten_layer = nn.Flatten()\n    dense_layer1 = nn.Linear(filter_list[-1] * (64 // (2**5))**2, dense_units)  # Assuming input size 64x64\n    dense_layer2 = nn.Linear(dense_units, class_count)\n    \n    return conv_net, flatten_layer, dense_layer1, dense_layer2\n\ndef select_activation(act_func):\n    if act_func == 'relu':\n        return nn.ReLU()\n    elif act_func == 'sigmoid':\n        return nn.Sigmoid()\n    elif act_func == 'tanh':\n        return nn.Tanh()\n    else:\n        raise ValueError(\"Unsupported activation function\")\n\ndef run_forward(data, conv_net, flatten_layer, dense_layer1, dense_layer2, act_func):\n    data = conv_net(data)\n    data = flatten_layer(data)\n    data = dense_layer1(data)\n    data = select_activation(act_func)(data)\n    data = dense_layer2(data)\n    return data\n\n# Example usage\nconv_net, flatten_layer, dense_layer1, dense_layer2 = build_cnn(\n    filter_list=[32, 64, 128, 256, 512],  # Filters per layer\n    kernel_dim=3,  # Kernel size\n    act_func='relu',  # Activation function\n    dense_units=256,  # Number of neurons in dense layer\n    class_count=10,  # 10 classes for classification\n    input_depth=3  # RGB images\n)\n\nprint(conv_net, dense_layer1, dense_layer2)","metadata":{"execution":{"iopub.execute_input":"2025-04-02T13:57:04.216495Z","iopub.status.busy":"2025-04-02T13:57:04.216129Z","iopub.status.idle":"2025-04-02T13:57:04.253148Z","shell.execute_reply":"2025-04-02T13:57:04.252077Z","shell.execute_reply.started":"2025-04-02T13:57:04.216449Z"}},"outputs":[{"name":"stdout","output_type":"stream","text":["Sequential(\n","  (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (1): ReLU()\n","  (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (4): ReLU()\n","  (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (7): ReLU()\n","  (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (9): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (10): ReLU()\n","  (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (12): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (13): ReLU()\n","  (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",") Linear(in_features=2048, out_features=256, bias=True) Linear(in_features=256, out_features=10, bias=True)\n"]}],"execution_count":2},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms as transforms\nimport wandb\nfrom torch.utils.data import DataLoader, random_split\n\nimport wandb\nwandb.login()\n\n\ndef build_cnn(filter_list, kernel_dim, act_func, dense_units, dropout_rate, batch_norm, class_count=10, input_depth=3):\n    network_layers = []\n    prev_channels = input_depth\n    \n    for num_filters in filter_list:\n        network_layers.append(nn.Conv2d(prev_channels, num_filters, kernel_size=kernel_dim, padding=1))\n        if batch_norm:\n            network_layers.append(nn.BatchNorm2d(num_filters))\n        network_layers.append(select_activation(act_func))\n        network_layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n        if dropout_rate > 0:\n            network_layers.append(nn.Dropout(dropout_rate))\n        prev_channels = num_filters\n    \n    conv_net = nn.Sequential(*network_layers)\n    flatten_layer = nn.Flatten()\n    dense_layer1 = nn.Linear(filter_list[-1] * (64 // (2**5))**2, dense_units)  # Assuming input size 64x64\n    dense_layer2 = nn.Linear(dense_units, class_count)\n    \n    return conv_net, flatten_layer, dense_layer1, dense_layer2\n\ndef select_activation(act_func):\n    activations = {\n        'relu': nn.ReLU(),\n        'gelu': nn.GELU(),\n        'silu': nn.SiLU(),\n        'mish': nn.Mish()\n    }\n    return activations.get(act_func, nn.ReLU())\n\ndef run_forward(data, conv_net, flatten_layer, dense_layer1, dense_layer2, act_func):\n    data = conv_net(data)\n    data = flatten_layer(data)\n    data = dense_layer1(data)\n    data = select_activation(act_func)(data)\n    data = dense_layer2(data)\n    return data\n\n# Initialize wandb\nwandb.init(project='cnn-hyperparam-sweep')\n\ndef train():\n    config = wandb.config\n    \n    # Load dataset\n    transform = transforms.Compose([\n        transforms.RandomHorizontalFlip() if config.data_aug else transforms.ToTensor(),\n        transforms.ToTensor()\n    ])\n    dataset = torchvision.datasets.ImageFolder(root='iNaturalist/train', transform=transform)\n    train_size = int(0.8 * len(dataset))\n    val_size = len(dataset) - train_size\n    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n    \n    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n    \n    conv_net, flatten_layer, dense_layer1, dense_layer2 = build_cnn(\n        filter_list=config.num_filters,\n        kernel_dim=config.kernel_size,\n        act_func=config.activation,\n        dense_units=config.dense_neurons,\n        dropout_rate=config.dropout,\n        batch_norm=config.batch_norm\n    )\n    \n    model = nn.Sequential(conv_net, flatten_layer, dense_layer1, nn.ReLU(), dense_layer2)\n    optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n    criterion = nn.CrossEntropyLoss()\n    \n    for epoch in range(10):\n        model.train()\n        for images, labels in train_loader:\n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n        \n        # Validation\n        model.eval()\n        correct, total = 0, 0\n        with torch.no_grad():\n            for images, labels in val_loader:\n                outputs = model(images)\n                _, predicted = torch.max(outputs, 1)\n                total += labels.size(0)\n                correct += (predicted == labels).sum().item()\n        \n        accuracy = 100 * correct / total\n        wandb.log({'val_accuracy': accuracy})\n        \n    return accuracy\n\n# Hyperparameter sweep configuration\nsweep_config = {\n    'method': 'random',\n    'metric': {'name': 'val_accuracy', 'goal': 'maximize'},\n    'parameters': {\n        'num_filters': {'values': [[32, 64, 128], [64, 128, 256]]},\n        'kernel_size': {'values': [3, 5]},\n        'activation': {'values': ['relu', 'gelu', 'silu', 'mish']},\n        'dense_neurons': {'values': [128, 256]},\n        'dropout': {'values': [0.2, 0.3]},\n        'batch_norm': {'values': [True, False]},\n        'data_aug': {'values': [True, False]},\n        'learning_rate': {'values': [0.001, 0.0001]}\n    }\n}\n\nsweep_id = wandb.sweep(sweep_config, project='cnn-hyperparam-sweep')\nwandb.agent(sweep_id, train, count=2)","metadata":{"execution":{"iopub.execute_input":"2025-04-02T13:40:18.653580Z","iopub.status.busy":"2025-04-02T13:40:18.653184Z","iopub.status.idle":"2025-04-02T13:56:58.341094Z","shell.execute_reply":"2025-04-02T13:56:58.339573Z","shell.execute_reply.started":"2025-04-02T13:40:18.653546Z"}},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"]},{"data":{"application/javascript":["\n","        window._wandbApiKey = new Promise((resolve, reject) => {\n","            function loadScript(url) {\n","            return new Promise(function(resolve, reject) {\n","                let newScript = document.createElement(\"script\");\n","                newScript.onerror = reject;\n","                newScript.onload = resolve;\n","                document.body.appendChild(newScript);\n","                newScript.src = url;\n","            });\n","            }\n","            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n","            const iframe = document.createElement('iframe')\n","            iframe.style.cssText = \"width:0;height:0;border:none\"\n","            document.body.appendChild(iframe)\n","            const handshake = new Postmate({\n","                container: iframe,\n","                url: 'https://wandb.ai/authorize'\n","            });\n","            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n","            handshake.then(function(child) {\n","                child.on('authorize', data => {\n","                    clearTimeout(timeout)\n","                    resolve(data)\n","                });\n","            });\n","            })\n","        });\n","    "],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{},"output_type":"display_data"},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-708a3e932478>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_login.py\u001b[0m in \u001b[0;36mlogin\u001b[0;34m(anonymous, key, relogin, host, force, timeout, verify)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m     configured = _login(\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0manonymous\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0manonymous\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_login.py\u001b[0m in \u001b[0;36m_login\u001b[0;34m(anonymous, key, relogin, host, force, timeout, _backend, _silent, _disable_warning, _entity)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mwlogin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompt_api_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwlogin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_key\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_login.py\u001b[0m in \u001b[0;36mprompt_api_key\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprompt_api_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0;34m\"\"\"Updates the global API key by prompting the user.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m         \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prompt_api_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mApiKeyStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNOTTY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m             directive = (\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_login.py\u001b[0m in \u001b[0;36m_prompt_api_key\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m                 key = apikey.prompt_api_key(\n\u001b[0m\u001b[1;32m    244\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_settings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m                     \u001b[0mapi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/apikey.py\u001b[0m in \u001b[0;36mprompt_api_key\u001b[0;34m(settings, api, input_callback, browser_callback, no_offline, no_create, local)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mjupyter\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"google.colab\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0mlog_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mterm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLOG_STRING_NOCOLOR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjupyter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattempt_colab_login\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapp_url\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0mwrite_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/jupyter.py\u001b[0m in \u001b[0;36mattempt_colab_login\u001b[0;34m(app_url)\u001b[0m\n\u001b[1;32m    334\u001b[0m     )\n\u001b[1;32m    335\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_js\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_wandbApiKey\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result, timeout_sec)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     if (\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"execution_count":1},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms as transforms\nimport wandb\nfrom torch.utils.data import DataLoader, random_split\n\ndef build_cnn(filter_list, kernel_dim, act_func, dense_units, dropout_rate, batch_norm, class_count=10, input_depth=3):\n    network_layers = []\n    prev_channels = input_depth\n    \n    for num_filters in filter_list:\n        network_layers.append(nn.Conv2d(prev_channels, num_filters, kernel_size=kernel_dim, padding=1))\n        if batch_norm:\n            network_layers.append(nn.BatchNorm2d(num_filters))\n        network_layers.append(select_activation(act_func))\n        network_layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n        if dropout_rate > 0:\n            network_layers.append(nn.Dropout(dropout_rate))\n        prev_channels = num_filters\n    \n    conv_net = nn.Sequential(*network_layers)\n    flatten_layer = nn.Flatten()\n    dense_layer1 = nn.Linear(filter_list[-1] * (64 // (2**5))**2, dense_units)  # Assuming input size 64x64\n    dense_layer2 = nn.Linear(dense_units, class_count)\n    \n    return conv_net, flatten_layer, dense_layer1, dense_layer2\n\ndef select_activation(act_func):\n    activations = {\n        'relu': nn.ReLU(),\n        'gelu': nn.GELU(),\n        'silu': nn.SiLU(),\n        'mish': nn.Mish()\n    }\n    return activations.get(act_func, nn.ReLU())\n\ndef train():\n    wandb.init(project='cnn-hyperparam-sweep')\n    config = wandb.config\n    \n    # Load dataset\n    transform_list = [transforms.ToTensor()]\n    if config.data_aug:\n        transform_list.insert(0, transforms.RandomHorizontalFlip())\n    transform = transforms.Compose(transform_list)\n    \n    dataset = torchvision.datasets.ImageFolder(root='iNaturalist/train', transform=transform)\n    train_size = int(0.8 * len(dataset))\n    val_size = len(dataset) - train_size\n    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n    \n    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n    \n    conv_net, flatten_layer, dense_layer1, dense_layer2 = build_cnn(\n        filter_list=config.num_filters,\n        kernel_dim=config.kernel_size,\n        act_func=config.activation,\n        dense_units=config.dense_neurons,\n        dropout_rate=config.dropout,\n        batch_norm=config.batch_norm\n    )\n    \n    model = nn.Sequential(conv_net, flatten_layer, dense_layer1, select_activation(config.activation), dense_layer2)\n    optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n    criterion = nn.CrossEntropyLoss()\n    \n    for epoch in range(10):\n        model.train()\n        for images, labels in train_loader:\n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n        \n        # Validation\n        model.eval()\n        correct, total = 0, 0\n        with torch.no_grad():\n            for images, labels in val_loader:\n                outputs = model(images)\n                _, predicted = torch.max(outputs, 1)\n                total += labels.size(0)\n                correct += (predicted == labels).sum().item()\n        \n        accuracy = 100 * correct / total\n        wandb.log({'val_accuracy': accuracy})\n    \n    wandb.finish()\n    return accuracy\n\n# Hyperparameter sweep configuration\nsweep_config = {\n    'method': 'random',\n    'metric': {'name': 'val_accuracy', 'goal': 'maximize'},\n    'parameters': {\n        'num_filters': {'values': [[32, 64, 128], [64, 128, 256]]},\n        'kernel_size': {'values': [3, 5]},\n        'activation': {'values': ['relu', 'gelu', 'silu', 'mish']},\n        'dense_neurons': {'values': [128, 256]},\n        'dropout': {'values': [0.2, 0.3]},\n        'batch_norm': {'values': [True, False]},\n        'data_aug': {'values': [True, False]},\n        'learning_rate': {'values': [0.001, 0.0001]}\n    }\n}\n\nsweep_id = wandb.sweep(sweep_config, project='cnn-hyperparam-sweep')\nwandb.agent(sweep_id, train, count=20)\n","metadata":{"execution":{"execution_failed":"2025-04-02T13:40:03.163Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}