# -*- coding: utf-8 -*-
"""ma23m026-a2-parta.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dnWe_6P2MzZlM7bpwrQjwdpiJrkxQgPM
"""

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All"
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

"""### Building a Small CNN Model for the iNaturalist Dataset

We will build a simple Convolutional Neural Network (CNN) model consisting of 5 convolution layers. Each convolution layer will be followed by an activation function and a max-pooling layer. After completing these 5 Conv-Activation-MaxPool blocks, the model will have one dense layer followed by an output layer containing 10 neurons, one for each of the 10 classes in the iNaturalist dataset.

#### Model Architecture
- **Input Layer**: Compatible with the images in the iNaturalist dataset (assuming the images are resized to 224x224x3).
- **Conv Layer 1**: Convolution followed by ReLU activation and max-pooling.
- **Conv Layer 2**: Convolution followed by ReLU activation and max-pooling.
- **Conv Layer 3**: Convolution followed by ReLU activation and max-pooling.
- **Conv Layer 4**: Convolution followed by ReLU activation and max-pooling.
- **Conv Layer 5**: Convolution followed by ReLU activation and max-pooling.
- **Dense Layer**: Flatten the output of the last convolution block and connect it to a dense layer.
- **Output Layer**: 10 neurons for the 10 classes, with a softmax activation function for classification.

Below is the implementation of the described model in PyTorch.
"""

import os
import numpy as np
import random
import wandb
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, Subset
from torchvision.datasets import ImageFolder
from torchvision import models
import matplotlib.pyplot as plt
from torchvision.utils import make_grid

"""## Wandb Login"""

wandb.login(key="acdc26d2fc17a56e83ea3ae6c10e496128dee648")

"""# CNN Class"""

# Defining a custom Convolutional Neural Network (CNN) class
class Cutomized_CNN(nn.Module):
    def __init__(
        self,
        input_channels=3,  # Number of input channels, e.g., 3 for RGB images
        conv_filters=[32, 64, 128, 256, 512],  # Number of filters for each conv layer
        kernel_sizes=[3, 3, 3, 3, 3],  # Kernel size for each conv layer
        activation_fn=F.relu,  # Activation function to be used after conv layers
        dense_neurons=256,  # Number of neurons in the fully connected hidden layer
        dense_activation_fn=F.relu,  # Activation function after dense layer
        dropout=0.0,  # Dropout rate (0 means no dropout)
        batch_norm=False,  # Whether to apply batch normalization after conv layers
        num_classes=10  # Number of output classes (for classification)
    ):
        # Initializing the parent nn.Module class
        super(Cutomized_CNN, self).__init__()

        # Saving configuration values
        self.activation_fn = activation_fn
        self.dense_activation_fn = dense_activation_fn
        self.dropout = dropout
        self.batch_norm = batch_norm

        # Initializing a list to hold all convolutional blocks
        self.conv_layers = nn.ModuleList()
        in_channels = input_channels  # Start with input channels (e.g., 3 for RGB)

        # Creating convolutional layers one by one
        for out_channels, kernel_size in zip(conv_filters, kernel_sizes):
            layers = []

            # Add convolutional layer with given kernel size and padding
            layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=1))

            # Optionally add batch normalization
            if batch_norm:
                layers.append(nn.BatchNorm2d(out_channels))

            # Add max pooling layer to reduce spatial dimensions
            layers.append(nn.MaxPool2d(kernel_size=2, stride=2))

            # Group the layers into a sequential block and add to the list
            self.conv_layers.append(nn.Sequential(*layers))

            # Update input channel count for the next conv layer
            in_channels = out_channels

        # Creating a dummy input tensor to compute the size after flattening
        self._dummy_input = torch.zeros(1, input_channels, 224, 224)

        # Compute the flattened size of the feature map after all conv layers
        self.flattened_size = self._get_flattennig_size()

        # Defining the first fully connected (dense) layer
        fc1_layers = []

        # Optionally add dropout before dense layer to reduce overfitting
        if dropout > 0:
            fc1_layers.append(nn.Dropout(dropout))

        # Add a linear layer from flattened input to dense_neurons
        fc1_layers.append(nn.Linear(self.flattened_size, dense_neurons))
        self.fc1 = nn.Sequential(*fc1_layers)

        # Final output layer from dense_neurons to number of classes
        self.fc2 = nn.Linear(dense_neurons, num_classes)

    # Helper function to calculate the flattened size after conv layers
    def _get_flattennig_size(self):
        x = self._dummy_input

        # Pass the dummy input through all conv blocks
        for block in self.conv_layers:
            for layer in block:
                if isinstance(layer, nn.Conv2d) or isinstance(layer, nn.BatchNorm2d):
                    x = layer(x)
                elif isinstance(layer, nn.MaxPool2d):
                    x = layer(x)

        # Flatten the output and return the number of features
        return x.view(1, -1).size(1)

    # forward pass of the network
    def forward_pass(self, x):
        # Pass the input through each conv block
        for block in self.conv_layers:
            for layer in block:
                if isinstance(layer, nn.Conv2d):
                    # Apply convolution and then activation
                    x = layer(x)
                    x = self.activation_fn(x)
                else:
                    # Apply either batch norm or max pooling
                    x = layer(x)

        # Flatten the output before feeding into dense layers
        x = x.view(x.size(0), -1)

        # Pass through first dense layer
        x = self.fc1(x)

        # Apply activation function
        x = self.dense_activation_fn(x)

        # Pass through final output layer
        x = self.fc2(x)

        # Return the final output (logits)
        return x

"""# Model Checking"""

# ----------------- Model Instantiation Example -----------------

# I am creating an instance of the Cutomized_CNN class by specifying
# various hyperparameters like number of input channels, convolution filters,
# kernel sizes, activation functions, dropout rate, and whether to apply batch normalization.

# input_channels=3        â†’ For RGB images (3 channels: Red, Green, Blue)
# conv_filters            â†’ List specifying number of filters in each conv layer
# kernel_sizes            â†’ List specifying the size of the kernel for each conv layer
# activation_fn           â†’ Activation function to apply after each conv layer (here, ReLU)
# dense_neurons=128       â†’ Number of neurons in the hidden dense (fully connected) layer
# dense_activation_fn     â†’ Activation function after the dense layer
# dropout=0.3             â†’ 30% dropout applied before the dense layer (helps prevent overfitting)
# batch_norm=True         â†’ Apply batch normalization after conv layers (stabilizes training)
# num_classes=10          â†’ Final output layer will have 10 units

model = Cutomized_CNN(
    input_channels=3,
    conv_filters=[16, 32, 64, 128, 256],
    kernel_sizes=[3, 3, 3, 3, 3],
    activation_fn=F.relu,
    dense_neurons=128,
    dense_activation_fn=F.relu,
    dropout=0.3,
    batch_norm=True,
    num_classes=10
)

# Print the architecture of the model to verify its structure
print(model)

"""### Training Model Using the iNaturalist Dataset

In this task, we will train a model using the iNaturalist dataset. The dataset consists of a **train** and a **test** folder. Here's the plan for splitting and training:

1. **Validation Split**: Set aside **20%** of the training data for validation purposes. Ensure that the validation data is **equally represented** for each class. The **test data** will not be used for hyperparameter tuning, as it's meant to evaluate the final model's performance.

2. **Hyperparameter Tuning Using wandb Sweep**: We will use **wandb**'s sweep feature to find the best hyperparameter configuration for our model. Below are some suggestions for hyperparameters that we will explore, but feel free to modify these as you see fit:
   - **Number of filters in each layer**: 32, 64, 128, etc.
   - **Activation function for the convolutional layers**: ReLU, GELU, SiLU, Mish, etc.
   - **Filter organization**: Same number of filters in all layers, doubling in each subsequent layer, halving in each subsequent layer, etc.
   - **Data Augmentation**: Yes, No.
   - **Batch Normalization**: Yes, No.
   - **Dropout**: 0.2, 0.3, etc. (It is important to decide where to apply dropout. Common places are after activation layers or before fully connected layers. Make sure to read up on this topic to determine the best location for dropout.)

3. **Sweep Setup**: Use wandb's sweep feature to explore these hyperparameters and find the best configuration. Here are the plots you should generate after completing the sweep:
   - **Accuracy vs. Experiment Count**: To visualize how many experiments you ran to identify the best configuration.
   - **Parallel Coordinates Plot**: To see how different hyperparameters are related to model performance.
   - **Correlation Summary Table**: To visualize the correlation of each hyperparameter with the final accuracy and loss.

4. **Reporting Results**:
   - After completing the sweep, include the plots generated by wandb as requested.
   - Write down the hyperparameters and their corresponding values that you experimented with.
   - Share any unique strategies you applied to reduce the number of experiments while still achieving a high accuracy. This could include methods like **grid search** or **random search**, **early stopping**, or **intelligent search spaces** for faster convergence.

Make sure to leverage **wandb's tracking features** for easy visualization and experiment management.

"""

# ------------- Activation map -------------------
# A dictionary mapping string names to PyTorch activation function modules.
# This allows dynamic selection of activation functions using a simple name.
activation_map = {
    "relu": nn.ReLU(),
    "gelu": nn.GELU(),
    "silu": nn.SiLU(),
    "mish": nn.Mish()
}

# Returns the chosen activation function wrapped in a lambda for usage.
def get_activation_function(name):
    return lambda x: activation_map[name.lower()](x)


# ------------- Dataset Loading and Split -------------------
# Loads image data from a directory and splits it into train/validation sets.
# Applies augmentation if required and returns PyTorch DataLoaders.
def get_dataloaders(data_dir, batch_size, val_split=0.2, augment=False):
    # Define transformation pipeline for training data
    transform_train = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.RandomHorizontalFlip(),   # Data augmentation - flips image
        transforms.RandomRotation(10),       # Data augmentation - rotates image
        transforms.ToTensor()
    ]) if augment else transforms.Compose([
        transforms.Resize((224, 224)),       # Only resizing and converting to tensor
        transforms.ToTensor()
    ])

    # Define transformation pipeline for validation data
    transform_val = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor()
    ])

    # Load the full dataset (from 'train' folder) using training transform
    full_dataset = ImageFolder(os.path.join(data_dir, 'train'), transform=transform_train)

    # Group indices of images by their class labels for stratified splitting
    label_to_indices = {}
    for idx, (_, label) in enumerate(full_dataset.samples):
        if label not in label_to_indices:
            label_to_indices[label] = []
        label_to_indices[label].append(idx)

    train_idx = []
    val_idx = []

    # Stratified split: maintain class distribution in both train and val sets
    for label in label_to_indices:
        indices = label_to_indices[label]
        random.shuffle(indices)
        split = int(len(indices) * val_split)
        val_idx.extend(indices[:split])
        train_idx.extend(indices[split:])

    # Create Subset objects for training and validation
    train_data = Subset(full_dataset, train_idx)
    val_data = Subset(ImageFolder(os.path.join(data_dir, 'train'), transform=transform_val), val_idx)

    # Wrap the subsets in DataLoaders
    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=2)
    val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False, num_workers=2)

    # Return loaders and number of classes
    return train_loader, val_loader, len(full_dataset.classes)


# ------------- Training and Evaluation Functions -------------------

# One training pass through the dataset (single epoch)
def train_epoch(model, optimizer, criterion, dataloader, device):
    model.train()  # Set model to training mode
    running_loss, correct, total = 0, 0, 0

    for images, labels in dataloader:
        images, labels = images.to(device), labels.to(device)

        optimizer.zero_grad()             # Clear gradients
        outputs = model(images)           # forward pass
        loss = criterion(outputs, labels) # Compute loss
        loss.backward()                   # Backward pass
        optimizer.step()                  # Update model parameters

        running_loss += loss.item()
        _, predicted = outputs.max(1)     # Get class predictions
        total += labels.size(0)
        correct += predicted.eq(labels).sum().item()  # Count correct predictions

    # Return average loss and accuracy for the epoch
    return running_loss / len(dataloader), 100. * correct / total


# Evaluation function (no gradient calculation)
def evaluate(model, criterion, dataloader, device):
    model.eval()  # Set model to evaluation mode
    loss, correct, total = 0, 0, 0

    with torch.no_grad():  # No gradient updates during evaluation
        for images, labels in dataloader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            loss += criterion(outputs, labels).item()
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()

    # Return average validation loss and accuracy
    return loss / len(dataloader), 100. * correct / total


# ------------- Train Loop for wandb Sweep -------------------
# Full training loop for use with wandb hyperparameter sweeps
def train(config=None):
    with wandb.init(config=config):  # Start a new wandb run
        config = wandb.config  # Get sweep config

        # ðŸ”½ Generate a unique name for each run based on config
        run_name = (
            f"filt-{config.base_filter}_{config.filter_organization}_"
            f"act-{config.activation_fn}_bn-{config.batch_norm}_"
            f"do-{config.dropout}_dense-{config.dense_neurons}_"
            f"bs-{config.batch_size}_lr-{config.lr}_aug-{config.data_augmentation}"
        )
        wandb.run.name = run_name

        # Set the device to GPU if available
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        # Load data loaders with optional augmentation
        train_loader, val_loader, num_classes = get_dataloaders(
            data_dir='/kaggle/input/inaturalist-dataset/inaturalist_12K',
            batch_size=config.batch_size,
            val_split=0.2,
            augment=config.data_augmentation
        )

        # Select filter sizes based on the selected strategy: same, double, or half
        conv_filters = {
            'same': [config.base_filter]*5,
            'double': [config.base_filter*(2**i) for i in range(5)],
            'half': [config.base_filter//(2**i) for i in range(5)],
        }[config.filter_organization]

        # Instantiate the CNN model with selected hyperparameters
        model = Cutomized_CNN(
            conv_filters=conv_filters,
            kernel_sizes=[3]*5,
            activation_fn=get_activation_function(config.activation_fn),
            dense_neurons=config.dense_neurons,
            dense_activation_fn=F.relu,
            dropout=config.dropout,
            batch_norm=config.batch_norm,
            num_classes=num_classes
        )

        model.to(device)  # Move model to GPU/CPU

        # Define loss function and optimizer
        criterion = nn.CrossEntropyLoss()
        optimizer = optim.Adam(model.parameters(), lr=config.lr)

        # Run training for specified number of epochs
        for epoch in range(config.epochs):
            train_loss, train_acc = train_epoch(model, optimizer, criterion, train_loader, device)
            val_loss, val_acc = evaluate(model, criterion, val_loader, device)

            # Print progress like Keras-style format
            print(f"Epoch : {epoch + 1}/{config.epochs}")
            print(f" -- Train Accuracy: {train_acc:.4f} -- Validation Accuracy: {val_acc:.4f}")

            # Log metrics to wandb for visualization
            wandb.log({
                "epoch": epoch + 1,
                "train_loss": train_loss,
                "train_acc": train_acc,
                "val_loss": val_loss,
                "val_acc": val_acc
            })

"""# Random Search with 10 Runs"""

# ------------- Sweep Config

sweep_config = {
    'method': 'random',
    'metric': {'name': 'val_acc', 'goal': 'maximize'},
    'parameters': {
        'base_filter': {'values': [32, 64]},
        'filter_organization': {'values': ['same', 'double', 'half']},
        'activation_fn': {'values': ['relu', 'gelu', 'silu', 'mish']},
        'data_augmentation': {'values': [True, False]},
        'batch_norm': {'values': [True, False]},
        'dropout': {'values': [0.2, 0.3]},
        'dense_neurons': {'values': [128, 256]},
        'batch_size': {'values': [32, 64, 128]},
        'lr': {'values': [1e-3, 1e-4]},
        'epochs': {'values': [5, 7, 10, 15, 17, 20]}
    }
}

sweep_id = wandb.sweep(sweep_config, project='iNaturalist-CNN')
wandb.agent(sweep_id, function=train, count = 10)

"""# Random Search with 50 Runs"""

# ------------- Sweep Config

sweep_config = {
    'method': 'random',
    'metric': {'name': 'val_acc', 'goal': 'maximize'},
    'parameters': {
        'base_filter': {'values': [32, 64]},
        'filter_organization': {'values': ['same', 'double', 'half']},
        'activation_fn': {'values': ['relu', 'gelu', 'silu', 'mish']},
        'data_augmentation': {'values': [True, False]},
        'batch_norm': {'values': [True, False]},
        'dropout': {'values': [0.2, 0.3]},
        'dense_neurons': {'values': [128, 256]},
        'batch_size': {'values': [32, 64, 128]},
        'lr': {'values': [1e-3, 1e-4]},
        'epochs': {'values': [5, 10, 15, 20]}
    }
}

sweep_id = wandb.sweep(sweep_config, project='iNaturalist-CNN-PartA-RandomSearch')
wandb.agent(sweep_id, function=train, count = 50)

"""# Bayes Search with 10 runs"""

# ------------- Sweep Config

sweep_config = {
    'method': 'bayes',
    'metric': {'name': 'val_acc', 'goal': 'maximize'},
    'parameters': {
        'base_filter': {'values': [32, 64]},
        'filter_organization': {'values': ['same', 'double']},
        'activation_fn': {'values': ['gelu', 'silu', 'mish']},
        'data_augmentation': {'values': [True]},
        'batch_norm': {'values': [True, False]},
        'dropout': {'values': [0.2, 0.3]},
        'dense_neurons': {'values': [128, 256]},
        'batch_size': {'values': [32, 64, 128]},
        'lr': {'values': [1e-3, 1e-4]},
        'epochs': {'values': [10, 15, 20]}
    }
}

sweep_id = wandb.sweep(sweep_config, project='iNaturalist-CNN-PartA-BayesianSearch')
wandb.agent(sweep_id, function=train, count = 10)

"""---

### Evaluate the Best Model on the Test Set

You will now apply your best model to the test data (Note: the test data should not have been used in any previous experiments, only train and validation data should have been used for model selection and evaluation).

- **Goal**: Use the best model from your sweep to report the accuracy on the test set.
  
---

### Displaying Sample Images and Predictions

Next, create a **10Ã—3 grid** displaying sample images from the test data along with predictions made by your best model. Make sure to present this grid creatively for better visualization and clarity.

---

# Test Data
"""

# Define the transformation for test images: resize and convert to tensor
transform_test = transforms.Compose([
    transforms.Resize((224, 224)),  # Resize images to 224x224
    transforms.ToTensor()           # Convert PIL images to PyTorch tensors
])

# Load the test dataset from the specified directory with the defined transformations
test_dataset = ImageFolder('/kaggle/input/inaturalist-dataset/inaturalist_12K/val', transform=transform_test)

# Create a DataLoader for the test dataset
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)  # No shuffle for evaluation

# Get the class names from the dataset (used for predictions or visualization)
class_names = test_dataset.classes

def plot_test_predictions(model, dataloader, class_names, device, num_images=30):
    model.eval()  # Set the model to evaluation mode
    images_shown = 0
    fig, axes = plt.subplots(10, 3, figsize=(12, 30))  # Create a 10x3 grid for displaying images
    axes = axes.flatten()  # Flatten the grid for easy indexing

    with torch.no_grad():  # Disable gradient calculation for inference
        for images, labels in dataloader:
            images, labels = images.to(device), labels.to(device)  # Move data to device
            outputs = model(images)  # Get model predictions
            _, preds = torch.max(outputs, 1)  # Get predicted class index

            for i in range(images.size(0)):
                img = images[i].cpu().permute(1, 2, 0).numpy()  # Convert image tensor to numpy for plotting
                label = class_names[labels[i]]  # Get ground truth class name
                pred = class_names[preds[i]]    # Get predicted class name

                axes[images_shown].imshow(img)  # Show image
                axes[images_shown].set_title(f"GT: {label}\nPred: {pred}")  # Annotate with GT and prediction
                axes[images_shown].axis('off')  # Turn off axis display

                images_shown += 1
                if images_shown == num_images:
                    plt.tight_layout()  # Adjust layout

                    # ðŸ”½ Log figure to wandb
                    wandb.log({"Test Predictions": wandb.Image(fig)})

                    plt.close(fig)  # Close plot to free memory
                    return  # Exit after logging desired number of images

def evaluate_test(model, dataloader, device):
    model.eval()
    correct, total = 0, 0
    with torch.no_grad():
        for images, labels in dataloader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            _, preds = outputs.max(1)
            correct += preds.eq(labels).sum().item()
            total += labels.size(0)
    print(f"Test Accuracy: {100 * correct / total:.2f}%")



# ------------- Activation map -------------------
# A dictionary mapping string names to PyTorch activation function modules.
# This allows dynamic selection of activation functions using a simple name.
activation_map = {
    "relu": nn.ReLU(),
    "gelu": nn.GELU(),
    "silu": nn.SiLU(),
    "mish": nn.Mish()
}

# Returns the chosen activation function wrapped in a lambda for usage.
def get_activation_function(name):
    return lambda x: activation_map[name.lower()](x)


# ------------- Dataset Loading and Split -------------------
# Loads image data from a directory and splits it into train/validation sets.
# Applies augmentation if required and returns PyTorch DataLoaders.
def get_dataloaders(data_dir, batch_size, val_split=0.2, augment=False):
    # Define transformation pipeline for training data
    transform_train = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.RandomHorizontalFlip(),   # Data augmentation - flips image
        transforms.RandomRotation(10),       # Data augmentation - rotates image
        transforms.ToTensor()
    ]) if augment else transforms.Compose([
        transforms.Resize((224, 224)),       # Only resizing and converting to tensor
        transforms.ToTensor()
    ])

    # Define transformation pipeline for validation data
    transform_val = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor()
    ])

    # Load the full dataset (from 'train' folder) using training transform
    full_dataset = ImageFolder(os.path.join(data_dir, 'train'), transform=transform_train)

    # Group indices of images by their class labels for stratified splitting
    label_to_indices = {}
    for idx, (_, label) in enumerate(full_dataset.samples):
        if label not in label_to_indices:
            label_to_indices[label] = []
        label_to_indices[label].append(idx)

    train_idx = []
    val_idx = []

    # Stratified split: maintain class distribution in both train and val sets
    for label in label_to_indices:
        indices = label_to_indices[label]
        random.shuffle(indices)
        split = int(len(indices) * val_split)
        val_idx.extend(indices[:split])
        train_idx.extend(indices[split:])

    # Create Subset objects for training and validation
    train_data = Subset(full_dataset, train_idx)
    val_data = Subset(ImageFolder(os.path.join(data_dir, 'train'), transform=transform_val), val_idx)

    # Wrap the subsets in DataLoaders
    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=2)
    val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False, num_workers=2)

    # Return loaders and number of classes
    return train_loader, val_loader, len(full_dataset.classes)


# ------------- Training and Evaluation Functions -------------------

# One training pass through the dataset (single epoch)
def train_epoch(model, optimizer, criterion, dataloader, device):
    model.train()  # Set model to training mode
    running_loss, correct, total = 0, 0, 0

    for images, labels in dataloader:
        images, labels = images.to(device), labels.to(device)

        optimizer.zero_grad()             # Clear gradients
        outputs = model(images)           # forward pass
        loss = criterion(outputs, labels) # Compute loss
        loss.backward()                   # Backward pass
        optimizer.step()                  # Update model parameters

        running_loss += loss.item()
        _, predicted = outputs.max(1)     # Get class predictions
        total += labels.size(0)
        correct += predicted.eq(labels).sum().item()  # Count correct predictions

    # Return average loss and accuracy for the epoch
    return running_loss / len(dataloader), 100. * correct / total


# Evaluation function (no gradient calculation)
def evaluate(model, criterion, dataloader, device):
    model.eval()  # Set model to evaluation mode
    loss, correct, total = 0, 0, 0

    with torch.no_grad():  # No gradient updates during evaluation
        for images, labels in dataloader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            loss += criterion(outputs, labels).item()
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()

    # Return average validation loss and accuracy
    return loss / len(dataloader), 100. * correct / total


# ------------- Train Loop for wandb Sweep -------------------
# Full training loop for use with wandb hyperparameter sweeps
def trainForTest(config=None):
    with wandb.init(config=config):  # Start a new wandb run
        config = wandb.config  # Get sweep config

        # ðŸ”½ Generate a unique name for each run based on config
        run_name = (
            f"filt-{config.base_filter}_{config.filter_organization}_"
            f"act-{config.activation_fn}_bn-{config.batch_norm}_"
            f"do-{config.dropout}_dense-{config.dense_neurons}_"
            f"bs-{config.batch_size}_lr-{config.lr}_aug-{config.data_augmentation}"
        )
        wandb.run.name = run_name

        # Set the device to GPU if available
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        # Load data loaders with optional augmentation
        train_loader, val_loader, num_classes = get_dataloaders(
            data_dir='/kaggle/input/inaturalist-dataset/inaturalist_12K',
            batch_size=config.batch_size,
            val_split=0.2,
            augment=config.data_augmentation
        )

        # Select filter sizes based on the selected strategy: same, double, or half
        conv_filters = {
            'same': [config.base_filter]*5,
            'double': [config.base_filter*(2**i) for i in range(5)],
            'half': [config.base_filter//(2**i) for i in range(5)],
        }[config.filter_organization]

        # Instantiate the CNN model with selected hyperparameters
        model = Cutomized_CNN(
            conv_filters=conv_filters,
            kernel_sizes=[3]*5,
            activation_fn=get_activation_function(config.activation_fn),
            dense_neurons=config.dense_neurons,
            dense_activation_fn=F.relu,
            dropout=config.dropout,
            batch_norm=config.batch_norm,
            num_classes=num_classes
        )

        model.to(device)  # Move model to GPU/CPU

        # Define loss function and optimizer
        criterion = nn.CrossEntropyLoss()
        optimizer = optim.Adam(model.parameters(), lr=config.lr)

        # Run training for specified number of epochs
        for epoch in range(config.epochs):
            train_loss, train_acc = train_epoch(model, optimizer, criterion, train_loader, device)
            val_loss, val_acc = evaluate(model, criterion, val_loader, device)


            # Print progress like Keras-style format
            print(f"Epoch : {epoch + 1}/{config.epochs}")
            print(f" -- Train Accuracy: {train_acc:.4f} -- Validation Accuracy: {val_acc:.4f}")


            # Log metrics to wandb for visualization
            wandb.log({
                "epoch": epoch + 1,
                "train_loss": train_loss,
                "train_acc": train_acc,
                "val_loss": val_loss,
                "val_acc": val_acc
            })

        evaluate_test(model, test_loader, device)
        plot_test_predictions(model, test_loader, class_names, device)

# filt-32_double_act-mish_bn-True_do-0.2_dense-256_bs-64_lr-0.0001_aug-True

sweep_config = {
    'method': 'random',
    'metric': {'name': 'val_acc', 'goal': 'maximize'},
    'parameters': {
        'base_filter': {'values': [32]},
        'filter_organization': {'values': ['double']},
        'activation_fn': {'values': ['mish']},
        'data_augmentation': {'values': [True]},
        'batch_norm': {'values': [True]},
        'dropout': {'values': [0.2]},
        'dense_neurons': {'values': [256]},
        'batch_size': {'values': [64]},
        'lr': {'values': [1e-4]},
        'epochs': {'values': [5]}
    }
}

sweep_id = wandb.sweep(sweep_config, project='iNaturalist-CNN-testnew1')
wandb.agent(sweep_id, function=trainForTest, count = 1)

"""---

### (Optional): Visualizing Filters in the First Layer

(Optional) Visualize the filters in the first convolutional layer of your best model for a random image from the test set. If the first layer has 64 filters, plot them in an **8Ã—8 grid** (64 filters). This will help understand what kind of features the model has learned.

---
"""

def visualize_first_layer_filters(model):
    # Extract filters from the first convolutional layer
    filters = model.conv_layers[0][0].weight.data.clone()

    # Normalize filters to range [0, 1] for visualization
    filters = (filters - filters.min()) / (filters.max() - filters.min())

    # Create a grid of filter images
    grid = make_grid(filters, nrow=8, normalize=True, padding=1)

    # Create a new figure for plotting
    plt.figure(figsize=(8, 8))

    # Set title for the plot
    plt.title("Conv1 Filters")

    # Display the filters (convert CHW to HWC for matplotlib)
    plt.imshow(grid.permute(1, 2, 0).cpu())

    # Remove axis ticks
    plt.axis('off')

    # Show the plot
    plt.show()

# Visualize the filters of the best model
visualize_first_layer_filters(best_model)

"""---

### (Optional): Guided Backpropagation on Neurons in CONV5 Layer

(Optional) Apply **guided backpropagation** on any **10 neurons** in the **CONV5 layer** of your model and plot the images that excite each of these neurons. These images can help you discover interesting patterns which excite specific neurons in the model. For this, plot the results in a **10Ã—1 grid** (10 neurons), where each image corresponds to one neuron.

---
"""

class GuidedBackprop:
    def __init__(self, model):
        self.model = model
        self.model.eval()
        self.gradients = None

        # Hook to modify gradient flow through ReLU (only allow positive gradients)
        def relu_hook_function(module, grad_in, grad_out):
            return (torch.clamp(grad_in[0], min=0.0),)

        # Register backward hook to all ReLU activations
        for module in self.model.modules():
            if isinstance(module, torch.nn.ReLU):
                module.register_backward_hook(relu_hook_function)

    def generate_gradients(self, input_image, target_layer_idx=4, filter_idx=0):
        # Enable gradient tracking for the input image
        input_image.requires_grad = True

        # forward pass up to the target convolutional layer
        x = input_image
        for i in range(target_layer_idx + 1):
            x = self.model.conv_layers[i](x)

        # Backward pass to compute gradients w.r.t. the input image
        x[:, filter_idx].mean().backward(retain_graph=True)

        # Process gradients for visualization
        grad = input_image.grad[0].cpu().permute(1, 2, 0).numpy()
        grad = (grad - grad.min()) / (grad.max() - grad.min())
        return grad

def plot_guided_backprop(model, dataloader, num_filters=10):
    # Create GuidedBackprop object
    gbp = GuidedBackprop(model)

    # Take the first image from the dataloader
    for images, _ in dataloader:
        image = images[0:1].to(device)
        break

    # Plot gradients for each filter in the specified layer
    fig, axes = plt.subplots(num_filters, 1, figsize=(5, num_filters * 3))
    for i in range(num_filters):
        grad = gbp.generate_gradients(image.clone(), filter_idx=i)
        axes[i].imshow(grad)
        axes[i].axis('off')
        axes[i].set_title(f"Neuron {i} in CONV5")

    # Adjust subplot spacing
    plt.tight_layout()
    plt.show()

# Visualize guided backpropagation for selected neurons
plot_guided_backprop(best_model, test_loader)

