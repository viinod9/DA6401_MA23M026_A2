# Avoid using triple backticks inside the multiline string to prevent syntax errors

readme_content = (
    "# CNN Classifier with W&B Hyperparameter Tuning\n"
    "Project File: `MA23M026_A2_PartA.ipynb`\n\n"
    "Hi, this is my notebook where I’ve built a custom Convolutional Neural Network (CNN) using PyTorch. "
    "I also connected it with Weights and Biases (W&B) to do a Bayesian hyperparameter sweep.\n"
    "This project is mainly designed to work with the iNaturalist 12K image classification dataset, but "
    "you can use any dataset with a similar folder structure.\n\n"
    "## Dataset Format\n\n"
    "Make sure your dataset directory looks like this:\n\n"
    "/inaturalist_12K/\n"
    "  └── train/\n"
    "      ├── class1/\n"
    "      ├── class2/\n"
    "      └── ...\n\n"
    "The code loads data from the `train/` folder and automatically splits it into training and validation sets.\n\n"
    "## Code Breakdown\n\n"
    "### 1. Custom CNN (Cutomized_CNN class)\n"
    "- Customizable number of conv filters, kernel sizes, activation functions (relu, gelu, silu, mish), "
    "dropout rate, batch normalization, dense layer size, and number of classes.\n"
    "- Includes a `forward_pass` method that runs input through conv layers, flattens it, and uses dense layers to output predictions.\n\n"
    "### 2. Activation Function Mapper\n"
    "- Uses a dictionary to map strings to PyTorch activation functions.\n\n"
    "### 3. Data Loading and Augmentation\n"
    "- Loads data using ImageFolder, applies optional augmentation (flip, rotate), stratified train/val split, and returns DataLoaders.\n\n"
    "### 4. Training and Evaluation\n"
    "- `train_epoch()` runs one epoch and computes loss and accuracy.\n"
    "- `evaluate()` checks model on validation set.\n\n"
    "### 5. Training with W&B Sweeps\n"
    "- Builds the model using values from W&B config.\n"
    "- Trains and logs metrics.\n\n"
    "### 6. Sweep Configuration\n"
    "- Uses Bayesian optimization to search for best hyperparameters.\n\n"
    "## How to Run\n"
    "1. Install Weights and Biases:\n"
    "   pip install wandb\n"
    "2. Login to W&B:\n"
    "   wandb login\n"
    "3. Place your dataset at:\n"
    "   /kaggle/input/inaturalist-dataset/inaturalist_12K/train/\n"
    "4. Run the sweep and training:\n"
    "   sweep_id = wandb.sweep(sweep_config, project='iNaturalist-CNN-PartA-BayesianSearch')\n"
    "   wandb.agent(sweep_id, function=train, count=10)\n\n"
    "## Summary\n"
    "Feature | Description\n"
    "--------|-------------\n"
    "Custom CNN | Built from scratch, easily configurable\n"
    "Stratified Split | Keeps class balance in train and val sets\n"
    "Data Augmentation | Optional transforms like flip and rotation\n"
    "Activation Function Map | Supports relu, gelu, silu, mish\n"
    "W&B Sweep Integration | Tracks and tunes hyperparameters automatically\n\n"
    "## Notes\n"
    "- Input image size is resized to 224x224\n"
    "- Uses CrossEntropyLoss and Adam optimizer\n"
    "- Works with both CPU and GPU\n"
    "- Code is modular and reusable for other datasets\n"
)

